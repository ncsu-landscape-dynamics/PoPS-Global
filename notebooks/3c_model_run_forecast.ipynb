{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoPS Global Model: Forecast \n",
    "Use this notebook to run the model with parameters sets sampled from a distribution generated from the previous model calibration step. These sampled parameter sets generate a forecast that propagates parameter uncertainty over multiple stochastic model runs.  \n",
    "\n",
    "This notebook can be run after 0, 1, 2, and 3b. We recommend also running 3a first, to check for and troubleshoot issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace from env and configuration files \n",
    "\n",
    "First, import needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to main repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed PoPS Global functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandemic.multirun_helpers import write_commands, generate_param_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in path variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables and paths from .env\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "# Read environmental variables\n",
    "input_dir = os.getenv(\"INPUT_PATH\")\n",
    "out_dir = os.getenv(\"OUTPUT_PATH\")\n",
    "sim_name = os.getenv(\"SIM_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in parameters from config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_json_path = f\"{out_dir}/config_{sim_name}.json\"\n",
    "\n",
    "with open(config_json_path) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "coi = config[\"coi\"]\n",
    "sim_years = config[\"sim_years\"]\n",
    "validation_method = config[\"validation_method\"]\n",
    "\n",
    "run_name = f\"{sim_name}_calibrate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the summary stats from the grid search to generate a parameter distribution\n",
    "\n",
    "Read summary statistics from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dir = f\"{out_dir}/summary_stats/{run_name}\"\n",
    "\n",
    "col_dict = {\n",
    "    \"start_max\": \"start\",\n",
    "    \"alpha_max\": \"alpha\",\n",
    "    \"beta_max\": \"beta\",\n",
    "    \"lamda_max\": \"lamda\",\n",
    "}\n",
    "\n",
    "agg_df = pd.read_csv(f\"{stats_dir}/summary_stats_bySample.csv\").rename(columns=col_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to save forecast figures\n",
    "\n",
    "fig_dir = f\"{stats_dir}/figs/forecast/\"\n",
    "\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a performance threshold\n",
    "Select a threhold percentile value (on F-beta) to determine which samples to use to fit the distribution. \n",
    "\n",
    "The viusalizations below help explore the possible thresholds and their impact on the number of samples included and the \n",
    "corresponding cut-off value for F-beta, and on the distribution of parameters included. \n",
    "\n",
    "If Leave-One-Out cross validation is used, this will produce a plot for each Fbeta (overall Fbeta, and one per location in the validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the fbeta columns\n",
    "fbeta_cols = [fbeta_col for fbeta_col in agg_df.columns if \"fbeta\" in fbeta_col and \"mean\" in fbeta_col]\n",
    "\n",
    "# Set up an empty dictionary of lists to store the results\n",
    "min_fbetas = {}\n",
    "min_fbetas[\"quantile\"] = []\n",
    "min_fbetas[\"count\"] = []\n",
    "\n",
    "for fbeta_col in fbeta_cols:\n",
    "    min_fbetas[fbeta_col] = []\n",
    "\n",
    "# Loop through the quantile thresholds\n",
    "for val in range(70, 100):\n",
    "    min_fbetas[\"quantile\"] += [val]\n",
    "    for fbeta_col in fbeta_cols:\n",
    "        subset = agg_df.loc[agg_df[fbeta_col] >= agg_df[fbeta_col].quantile(val / 100)]   \n",
    "        min_fbetas[fbeta_col] += [subset[fbeta_col].min()]\n",
    "    # Count is consistent across metrics\n",
    "    min_fbetas[\"count\"] += [len(subset.index)]     \n",
    "\n",
    "# Convert to dataframe\n",
    "\n",
    "sample_stats = pd.DataFrame(\n",
    "    min_fbetas\n",
    "    ).set_index(\"quantile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a % threshold (0 - 100) - adjust based on the below plots\n",
    "\n",
    "quant_threshold = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual: How many samples and what Fbeta scores are captured with each threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = len(fbeta_cols) + 1\n",
    "fig, axs = plt.subplots(1, width, figsize=(4*width, 4))\n",
    "\n",
    "sample_stats[\"count\"].plot(ax=axs[0])\n",
    "axs[0].vlines(\n",
    "    quant_threshold,\n",
    "    ymin=sample_stats[\"count\"].min(),\n",
    "    ymax=sample_stats[\"count\"].max(),\n",
    "    linestyle=\"dashed\",\n",
    "    color=\"firebrick\",\n",
    ")\n",
    "axs[0].set_title(f\"Count\")\n",
    "\n",
    "for i, fbeta_col in enumerate(fbeta_cols):\n",
    "\n",
    "    sample_stats[fbeta_col].plot(ax=axs[i+1])\n",
    "    axs[i+1].vlines(\n",
    "        quant_threshold,\n",
    "        ymin=sample_stats[fbeta_col].min(),\n",
    "        ymax=sample_stats[fbeta_col].max(),\n",
    "        linestyle=\"dashed\",\n",
    "        color=\"firebrick\",\n",
    "    )\n",
    "    axs[i+1].set_title(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}\")\n",
    "\n",
    "plt.savefig(f\"{fig_dir}/sample_threshold.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual: What do the distributions of alpha and lamda look like with that threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fbeta_col in fbeta_cols:\n",
    "    agg_df[f\"top_{fbeta_col}\"] = np.where(\n",
    "        agg_df[fbeta_col] >= agg_df[fbeta_col].quantile(quant_threshold / 100), \"top\", \"low\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize separation by parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha by year\n",
    "for fbeta_col in fbeta_cols:\n",
    "    ax = sns.relplot(\n",
    "        x=\"alpha\",\n",
    "        y=fbeta_col,\n",
    "        col=\"start\",\n",
    "        hue=f\"top_{fbeta_col}\",\n",
    "        palette=\"rocket\",\n",
    "        data=agg_df,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        s=100,\n",
    "    )\n",
    "    plt.savefig(f\"{fig_dir}/top_alpha_{fbeta_col}_start.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lamda by year\n",
    "for fbeta_col in fbeta_cols:\n",
    "    ax = sns.relplot(\n",
    "        x=\"lamda\",\n",
    "        y=fbeta_col,\n",
    "        col=\"start\",\n",
    "        hue=f\"top_{fbeta_col}\",\n",
    "        palette=\"rocket\",\n",
    "        data=agg_df,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        s=100,\n",
    "    )\n",
    "\n",
    "    plt.savefig(f\"{fig_dir}/top_lambda_{fbeta_col}_start.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta by year\n",
    "for fbeta_col in fbeta_cols:\n",
    "    ax = sns.relplot(\n",
    "        x=\"beta\",\n",
    "        y=fbeta_col,\n",
    "        col=\"start\",\n",
    "        hue=f\"top_{fbeta_col}\",\n",
    "        palette=\"rocket\",\n",
    "        data=agg_df,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        s=100,\n",
    "    )\n",
    "\n",
    "    plt.savefig(f\"{fig_dir}/top_beta_{fbeta_col}_start.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the overall parameter distributions of sampled sets. \n",
    "\n",
    "- If validation method is \"loo\", the sampled parameters will be fit from parameter sets above the quantile threshold for each omitted validation location's Fbeta. Parameter sets that appear above this threshold for multiple locations will be repeated in the set.\n",
    "- If validation method is none, the sampled parameters will be fit from the overall sample Fbeta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset of the top samples\n",
    "\n",
    "if validation_method == \"loo\":\n",
    "    # Eliminate the sample fbeta column\n",
    "    fbeta_cols = [fbeta_col for fbeta_col in fbeta_cols if \"no\" in fbeta_col]\n",
    "\n",
    "top_samples = pd.DataFrame()\n",
    "\n",
    "for fbeta_col in fbeta_cols:\n",
    "    top_samples = pd.concat(\n",
    "        [\n",
    "            top_samples,\n",
    "            (\n",
    "                agg_df.loc[agg_df[f\"top_{fbeta_col}\"] == \"top\", \n",
    "                [\"start\", \"alpha\", \"beta\", \"lamda\", fbeta_col]]\n",
    "                .rename(columns={fbeta_col: \"fbeta\"})\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "top_samples = top_samples.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top parameter distribution plot\n",
    "ax = sns.relplot(\n",
    "    x=\"alpha\",\n",
    "    y=\"lamda\",\n",
    "    col=\"start\",\n",
    "    hue=\"fbeta\",\n",
    "    palette=\"mako_r\",\n",
    "    data=top_samples,\n",
    ")\n",
    "\n",
    "plt.savefig(f\"{fig_dir}/top_param_distributions.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a multivariate normal distribution and sampled parameters\n",
    "Using the samples above your threshold, randomly sample a set of new parameter sets from their distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many distinct parameter samples do you want to generate?\n",
    "n_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits a separate distribution per year\n",
    "\n",
    "samples_to_run = generate_param_samples(top_samples, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sampled parameters to .csv as a backup/for later use\n",
    "\n",
    "samples_to_run.to_csv(f\"{stats_dir}/sampled_param_sets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the parameter distributions that will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to visually examine the parameter posterior distributions\n",
    "\n",
    "ax = sns.jointplot(\n",
    "    x=\"alpha\", y=\"lamda\", hue=\"start\", data=samples_to_run, palette=\"deep\", alpha=0.6\n",
    ")\n",
    "plt.savefig(f\"{fig_dir}/posterior_param_dist.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model forecast\n",
    "\n",
    "First write out the commands with the new sampled parameter sets. One run will be conducted with each parameter sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands_forecast = \"\"\n",
    "\n",
    "for index, row in samples_to_run.iterrows():\n",
    "    commands_forecast += write_commands(\n",
    "        row, start_run=0, end_run=0, run_type=\"forecast\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you will run on HPC or later, write these to file\n",
    "\n",
    "f1 = open(stats_dir + \"/commands.txt\", 'w')\n",
    "f1.write(commands_forecast)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to execute all model runs. These must complete before you can calculate \n",
    "the summary statistics. Remember that this may take some time (approximately 2 - 5 minutes \n",
    "per run per core, depending on your computer and number of time-steps in your simulation), \n",
    "so prepare accordingly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model from script\n",
    "\n",
    "for command in commands_forecast.split('\\n'):\n",
    "    ! {command}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These runs will write out to \"outputs/{run_name}_forecast/\". \n",
    "\n",
    "Calculate summary statistics on completed runs. This is also run in parallel, so time \n",
    "will vary depending on how many cores you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary stats\n",
    "# Note: The summary stats  may generate a \"warning\" from the pandas library. This should not cause any errors.\n",
    "\n",
    "! python pandemic/get_stats.py forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review model summary statistics\n",
    "\n",
    "You can summarize the model runs now with a single set of summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{sim_name}_forecast\"\n",
    "stats_dir = f\"{out_dir}/summary_stats/{run_name}\"\n",
    "\n",
    "agg_df = pd.read_csv(f\"{stats_dir}/summary_stats_bySample.csv\").rename(columns=col_dict)\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Final forecast summary results: \\n\\n\"\n",
    "    f\"F-beta = {round(agg_df.loc[0,'fbeta_mean'],4)}\"\n",
    ")\n",
    "\n",
    "for year in sim_years:\n",
    "    print(\n",
    "        f\"Probability of intro. to {coi} by {year}: \"\n",
    "        f\"{round(agg_df.loc[0, [col for col in agg_df.columns if f'prob_by_{year}' in col]].values[0],4)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step: Visualize forecast\n",
    "\n",
    "Use notebook 4 to visualize the full results of your forecast simulation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5263cf785d9856e1c25f07618a8816d1a19b937a7dc164fd109bb133e6c86867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
