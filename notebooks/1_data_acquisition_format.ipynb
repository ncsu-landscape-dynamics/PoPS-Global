{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Formatting\n",
    "\n",
    "This notebook provides the workflow for aquiring and formatting the data needed to run the PoPS Global Model. To run this notebook, the following are assumed:\n",
    "- Data are saved in a project folder (e.g., Google Drive root H:/Project Folder/)\n",
    "- Cloned the PoPS Global GitHub repository (git clone https://github.com/ncsu-landscape-dynamics/PoPS-Global.git)\n",
    "- Notebook was launched from the notebook folder of the cloned repo\n",
    "- Already have the following data available or downloaded:\n",
    "    - Koppen-Geiger Climate Classification raster (e.g., Beck_KG_V1_present_0p083.tif from http://koeppen-geiger.vu-wien.ac.at/data)\n",
    "    - Phytosanitary capacity (data frame with country name and ISO3 code, estimate/index of phytosanitary capacity)\n",
    "    - Binary host map raster\n",
    "    - File with the following environmental variables:\n",
    "        - DATA_PATH (file path to data folder that will contain original data, formatted model input data, and model output data)\n",
    "        - COMTRADE_AUTH_KEY (API key to query and download data from the UN Comtrade Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "import dotenv\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook was launched from notebook folder of the clone GitHub\n",
    "# repository, then set working directory to level above\n",
    "# (e.g., '..' to navigate to /PoPS-Global)\n",
    "\n",
    "# This should be the path where the .env file is saved\n",
    "repo_path = \"..\"\n",
    "os.chdir(repo_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.Comtrade.get_comtrade import query_comtrade\n",
    "from pandemic.generate_trade_forecasts import simple_trade_forecast\n",
    "from pandemic.helpers import distance_between, convert_to_binary\n",
    "from pandemic.ecological_calculations import (\n",
    "    create_climate_similarities_matrix,\n",
    "    create_climate_similarities_matrix_origins,\n",
    ")\n",
    "from Data.GBIF import get_GBIF_key, get_GBIF_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environmental Variables and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables and paths from .env\n",
    "dotenv.load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to case-study specific raw data inputs (host map, phytosanitary capacity dataset)\n",
    "data_dir = os.getenv(\"DATA_PATH\")\n",
    "\n",
    "# Path to formatted model inputs\n",
    "input_dir = os.getenv(\"INPUT_PATH\")\n",
    "\n",
    "# Path to save outputs\n",
    "out_dir = os.getenv(\"OUTPUT_PATH\")\n",
    "\n",
    "# Simulation name\n",
    "sim_name = os.getenv(\"SIM_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UN Comtrade Data\n",
    "\n",
    "This step only needs to be run once per aggregation (e.g., monthly, annual, start year,\n",
    "commodity code). \n",
    "\n",
    "It queries the UN Comtrade API to download data based on the first year\n",
    "of interest, end year (inclusive), commodity codes, frequency (e.g., monthly, annual), \n",
    "and unit value (e.g., value in dollars or net weight). Data are saved as csvs by HS code\n",
    "and time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comtrade_auth_key = os.getenv(\"COMTRADE_AUTH_KEY\")\n",
    "start_year = 2000\n",
    "end_year = 2020\n",
    "temporal_res = \"M\"\n",
    "hs_list = [\"6802\", \"6803\"]\n",
    "agg_commodities = True\n",
    "\n",
    "if agg_commodities:\n",
    "    dir_suffix = \"agg\"\n",
    "else:\n",
    "    dir_suffix = \"adjusted\"\n",
    "\n",
    "if len(hs_list) == 1:\n",
    "    code_str = str(hs_list[0])\n",
    "else:\n",
    "    code_str = \"-\".join([str(hs_code) for hs_code in hs_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_comtrade(\n",
    "    model_inputs_dir=f\"{input_dir}/comtrade\",\n",
    "    auth_code=comtrade_auth_key,\n",
    "    hs_list=hs_list,\n",
    "    start_year=start_year,\n",
    "    end_year=end_year,\n",
    "    temporal_res=temporal_res,\n",
    "    crosswalk_path=\"Data/un_to_iso.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Trade Data for Inflation\n",
    "\n",
    "Get the Consumer Price Index from the US Bureau of Labor Statistics\n",
    "\n",
    "Series CUUR0000SA0L1E - All items less food and energy in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "\n",
    "Other CPI series are available. See more information here: https://www.bls.gov/cpi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CPI data\n",
    "# API allows 10 years per request, divide year requests into lists of 10 or less\n",
    "years = list(range(start_year, end_year + 1))\n",
    "year_subsets = [years[x : x + 10] for x in range(0, len(years), 10)]\n",
    "cpi_series = \"CUUR0000SA0L1E\"\n",
    "\n",
    "cpi_list = []\n",
    "for subset in year_subsets:\n",
    "    print(f\"Downloading CPI for {str(subset[0])}-{str(subset[-1])}...\")\n",
    "    headers = {\"Content-type\": \"application/json\"}\n",
    "    data = json.dumps(\n",
    "        {\n",
    "            \"seriesid\": [cpi_series],\n",
    "            \"startyear\": str(subset[0]),\n",
    "            \"endyear\": str(subset[-1]),\n",
    "        }\n",
    "    )\n",
    "    p = requests.post(\n",
    "        \"https://api.bls.gov/publicAPI/v1/timeseries/data/\", data=data, headers=headers\n",
    "    )\n",
    "    json_data = json.loads(p.text)\n",
    "    json_data = json_data[\"Results\"][\"series\"][0]\n",
    "\n",
    "    for ts in json_data[\"data\"]:\n",
    "        year = ts[\"year\"]\n",
    "        period = ts[\"period\"]\n",
    "        value = ts[\"value\"]\n",
    "        cpi_list.append([year, period, value])\n",
    "\n",
    "cpi_df = pd.DataFrame(cpi_list, columns=[\"year\", \"period\", \"cpi\"])\n",
    "cpi_df[\"period\"] = cpi_df[\"period\"].str.lstrip(\"M\")\n",
    "cpi_df[\"ts\"] = cpi_df[\"year\"] + cpi_df[\"period\"]\n",
    "cpi_df = cpi_df.set_index(\"ts\")\n",
    "print(f\"CPI for {len(cpi_df)} timesteps downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose baseline year to convert current (nominal) $ to\n",
    "base_ts = \"201901\"\n",
    "cpi_base = cpi_df.loc[base_ts, \"cpi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if temporal_res == \"M\":\n",
    "    file_list = glob.glob(f\"{input_dir}/comtrade/monthly/*/*.csv\")\n",
    "    if not file_list:\n",
    "        print(\"No trade files found...\")\n",
    "    else:\n",
    "        print(f\"Converting current $ to {base_ts} $ for {len(file_list)} files...\")\n",
    "        for file in file_list:\n",
    "            file_name = file.split(\"\\\\\")[-1]\n",
    "            ts = file_name[-10:-4]\n",
    "            cpi_ts = cpi_df.loc[ts, \"cpi\"]\n",
    "            adjusted_dir = f\"{input_dir}/comtrade/monthly_adjusted/{file_name[:4]}\"\n",
    "            if not os.path.exists(adjusted_dir):\n",
    "                os.makedirs(adjusted_dir)\n",
    "            trade = pd.read_csv(file, index_col=0)\n",
    "            trade_adjusted = (trade * (float(cpi_base) / 100)) / (float(cpi_ts) / 100)\n",
    "            trade_adjusted.to_csv(adjusted_dir + \"/\" + file_name)\n",
    "        print(f\"Adjusted trade values saved at {input_dir}/comtrade/monthly_adjusted/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Multiple Commodities\n",
    "\n",
    "This step only needs to be run once per download if the model will be run\n",
    "using a sum of all commodities of interest as opposed to by each commodity individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agg_commodities:\n",
    "    print(code_str)\n",
    "\n",
    "    if temporal_res == \"M\":\n",
    "        # get all trade dates\n",
    "        file_list = glob.glob(\n",
    "            f\"{input_dir}/comtrade/monthly_adjusted/{str(hs_list[0])}/{str(hs_list[0])}*.csv\"\n",
    "        )\n",
    "        date_list = []\n",
    "        for fn in file_list:\n",
    "            date = os.path.splitext(fn)[0].split(\"_\")[-1]\n",
    "            date_list.append(date)\n",
    "\n",
    "        date_list_unique = list(set(date_list))\n",
    "        date_list_unique.sort()\n",
    "\n",
    "        out_path = f\"{input_dir}/comtrade/monthly_agg/{code_str}/\"\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "\n",
    "        for d in date_list_unique:\n",
    "            d_file_list = glob.glob(\n",
    "                input_dir + f\"/comtrade/monthly_adjusted/*/*{d}*.csv\"\n",
    "            )\n",
    "            for use_hs in hs_list[1:]:\n",
    "                d_file_list += glob.glob(\n",
    "                    input_dir + f\"/comtrade/monthly_adjusted/{use_hs}/*{d}*.csv\"\n",
    "                )\n",
    "            print(f\"{d}: {len(d_file_list)}\")\n",
    "            dfs = [\n",
    "                pd.read_csv(f, sep=\",\", header=0, index_col=0, encoding=\"latin1\")\n",
    "                for f in d_file_list\n",
    "            ]\n",
    "            all_com = reduce(pd.DataFrame.add, dfs)\n",
    "            all_com.to_csv(out_path + f\"{code_str}_{d}.csv\")\n",
    "\n",
    "    # If trade data are annual\n",
    "    if temporal_res == \"A\":\n",
    "        out_path = input_dir + f\"/comtrade/annual_agg/{code_str}/\"\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "\n",
    "        year_range = list(range(start_year, end_year + 1, 1))\n",
    "        for d in year_range:\n",
    "            d_file_list = glob.glob(input_dir + f\"/comtrade/annual/*/*{d}.csv\")\n",
    "            print(f\"{d}: {len(d_file_list)}\")\n",
    "            dfs = [\n",
    "                pd.read_csv(f, sep=\",\", header=0, index_col=0, encoding=\"latin1\")\n",
    "                for f in d_file_list\n",
    "            ]\n",
    "            all_com = reduce(pd.DataFrame.add, dfs)\n",
    "            all_com.to_csv(out_path + f\"{code_str}_{d}.csv\")\n",
    "\n",
    "    if temporal_res == \"M\":\n",
    "        out_path = input_dir + f\"/comtrade/annual_agg/{code_str}/\"\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "\n",
    "        year_range = list(range(start_year, end_year + 1, 1))\n",
    "        for d in year_range:\n",
    "            d_file_list = glob.glob(\n",
    "                input_dir + f\"/comtrade/monthly_agg/{code_str}/*_{d}*.csv\"\n",
    "            )\n",
    "            print(f\"{d}: {len(d_file_list)}\")\n",
    "            dfs = [\n",
    "                pd.read_csv(f, sep=\",\", header=0, index_col=0, encoding=\"latin1\")\n",
    "                for f in d_file_list\n",
    "            ]\n",
    "            all_com = reduce(pd.DataFrame.add, dfs)\n",
    "            all_com.to_csv(out_path + f\"{code_str}_{d}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trade Forecast\n",
    "\n",
    "This step only needs to be run once per aggregation. It is a simple sampling of\n",
    "historical trade data to be used as predictions of future trade values.\n",
    "\n",
    "TO DO: Add ability to include a percent change (e.g., 1% increase) by year or time\n",
    "horizon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agg_commodities:\n",
    "    code_str = \"-\".join([str(hs_code) for hs_code in hs_list])\n",
    "    print(f\"Creating forecast with aggregated commodities: {code_str}\")\n",
    "    hist_trade_dir = f\"{input_dir}/comtrade/monthly_{dir_suffix}/{code_str}\"\n",
    "    forecast_dir = (\n",
    "        f\"{input_dir}/comtrade/trade_forecast/monthly_{dir_suffix}/{code_str}\"\n",
    "    )\n",
    "    start_forecast_year = (end_year + 1) * 100 + 1\n",
    "    number_historical_years = 5\n",
    "    number_forecast_years = 2\n",
    "    random_seed = None\n",
    "    simple_trade_forecast(\n",
    "        data_dir=f\"{input_dir}/comtrade\",\n",
    "        output_dir=forecast_dir,\n",
    "        start_forecast_date=start_forecast_year,\n",
    "        num_yrs_historical=number_historical_years,\n",
    "        num_yrs_forecast=number_forecast_years,\n",
    "        hist_data_dir=hist_trade_dir,\n",
    "        random_seed=random_seed,\n",
    "    )\n",
    "else:\n",
    "    for code in hs_list:\n",
    "        code_str = str(code)\n",
    "        print(f\"Creating forecast for {code_str}\")\n",
    "        hist_trade_dir = f\"{input_dir}/comtrade/monthly_{dir_suffix}/{code_str}\"\n",
    "        forecast_dir = (\n",
    "            f\"{input_dir}/comtrade/trade_forecast/monthly_{dir_suffix}/{code_str}\"\n",
    "        )\n",
    "        start_forecast_year = (end_year + 1) * 100 + 1\n",
    "        number_historical_years = 5\n",
    "        number_forecast_years = 2\n",
    "        random_seed = None\n",
    "        simple_trade_forecast(\n",
    "            data_dir=f\"{input_dir}/comtrade\",\n",
    "            output_dir=forecast_dir,\n",
    "            start_forecast_date=start_forecast_year,\n",
    "            num_yrs_historical=number_historical_years,\n",
    "            num_yrs_forecast=number_forecast_years,\n",
    "            hist_data_dir=hist_trade_dir,\n",
    "            random_seed=random_seed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating annual trade forecast data from the monthly forecast\n",
    "if temporal_res == \"M\":\n",
    "    if agg_commodities:\n",
    "        code_str = \"-\".join([str(hs_code) for hs_code in hs_list])\n",
    "        print(f\"{code_str}\")\n",
    "        out_path = input_dir + f\"/comtrade/trade_forecast/annual_agg/{code_str}/\"\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        forecast_year = int(str(start_forecast_year)[:4])\n",
    "        year_range = list(\n",
    "            range(forecast_year, forecast_year + number_forecast_years, 1)\n",
    "        )\n",
    "        for d in year_range:\n",
    "            d_file_list = glob.glob(\n",
    "                input_dir\n",
    "                + f\"/comtrade/trade_forecast/monthly_agg/{code_str}/*_{d}*.csv\"\n",
    "            )\n",
    "            print(f\"{d}: {len(d_file_list)}\")\n",
    "            dfs = [\n",
    "                pd.read_csv(f, sep=\",\", header=0, index_col=0, encoding=\"latin1\")\n",
    "                for f in d_file_list\n",
    "            ]\n",
    "            all_com = reduce(pd.DataFrame.add, dfs)\n",
    "            all_com.to_csv(out_path + f\"{code_str}_{d}.csv\")\n",
    "    else:\n",
    "        for code in hs_list:\n",
    "            code_str = code\n",
    "            print(f\"{code_str}\")\n",
    "            out_path = input_dir + f\"/comtrade/trade_forecast/annual_agg/{code_str}/\"\n",
    "            if not os.path.exists(out_path):\n",
    "                os.makedirs(out_path)\n",
    "            forecast_year = int(str(start_forecast_year)[:4])\n",
    "            year_range = list(\n",
    "                range(forecast_year, forecast_year + number_forecast_years, 1)\n",
    "            )\n",
    "            for d in year_range:\n",
    "                d_file_list = glob.glob(\n",
    "                    input_dir\n",
    "                    + f\"/comtrade/trade_forecast/monthly_adjusted/{code_str}/*_{d}*.csv\"\n",
    "                )\n",
    "                print(f\"{d}: {len(d_file_list)}\")\n",
    "                dfs = [\n",
    "                    pd.read_csv(f, sep=\",\", header=0, index_col=0, encoding=\"latin1\")\n",
    "                    for f in d_file_list\n",
    "                ]\n",
    "                all_com = reduce(pd.DataFrame.add, dfs)\n",
    "                all_com.to_csv(out_path + f\"{code_str}_{d}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_geo_path = \"Data/TM_WORLD_BORDERS-0.3/TM_WORLD_BORDERS-0.3.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_gdf = gpd.read_file(countries_geo_path)\n",
    "countries_gdf.iloc[136, 4] = \"Macao\"\n",
    "countries_gdf.iloc[169, 4] = \"Réunion\"\n",
    "countries_gdf.iloc[17, 4] = \"Myanmar\"\n",
    "countries_gdf.iloc[245, 4] = \"Saint Barthelemy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koppen-Geiger Climate Classification by Country\n",
    "This step only needs to be run once. \n",
    "\n",
    "It creates a data frame consisting of countries as rows, climate\n",
    "classification codes as columns, and percent area in each country\n",
    "as values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose mask to limit host and climate calculations to potential commodity destination areas\n",
    "# Options: 'hii' for Human Influence Index (e.g., urban areas), or\n",
    "# 'cl' for cropland areas (e.g., agricultural input)\n",
    "# or \"none\" to not mask out any areas\n",
    "\n",
    "mask = \"hii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koppen_path = \"Data/Beck_KG_V1_present_0p083.tif\"\n",
    "koppen_codes = pd.read_csv(\"Data/KGcodes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Koppen raster\n",
    "koppen_rast = rasterio.open(koppen_path)\n",
    "koppen_arr = koppen_rast.read(1)\n",
    "koppen_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask == \"cl\":\n",
    "    # If using cropland layer to mask host and climate areas\n",
    "    # Read cropland binary\n",
    "    cl = rasterio.open(f\"Data/cropland_binary.tif\")\n",
    "    cl_arr = cl.read(1)\n",
    "    cl.shape\n",
    "\n",
    "    # Set values\n",
    "    threshold_val = \"\"\n",
    "\n",
    "    # Mask selected areas\n",
    "    koppen = cl_arr[:2160, :4320] * koppen_arr\n",
    "\n",
    "if mask == \"hii\":\n",
    "    # Read Human Influence Index layer (already resampled to match Koppen raster)\n",
    "    hii = rasterio.open(\"Data/hii_v2_resamp.tif\")\n",
    "    hii_arr = hii.read(1)\n",
    "    hii_arr = hii_arr.astype(\"float64\")\n",
    "    hii_arr.shape\n",
    "\n",
    "    # Create mask to exclude areas with values below the threshold\n",
    "    # from the % area calculations of climate similaritiy and\n",
    "    # host availability\n",
    "    threshold_val = 16\n",
    "\n",
    "    # Mask values less than threshold (water is already 255)\n",
    "    hii_arr[hii_arr < threshold_val] = 0\n",
    "    hii_arr[hii_arr == 255] = 0\n",
    "\n",
    "    # Keep areas greater than threshold\n",
    "    hii_arr[hii_arr >= threshold_val] = 1\n",
    "    hii_arr[hii_arr == 0] = np.nan\n",
    "\n",
    "    # Mask selected areas\n",
    "    koppen = hii_arr * koppen_arr\n",
    "\n",
    "if mask == \"none\":\n",
    "    koppen = koppen_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate zonal statistics of koppen classes for each country\n",
    "affine = koppen_rast.transform\n",
    "stats = zonal_stats(countries_gdf, koppen, categorical=True, affine=affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add % of each climate classification to countries geodataframe\n",
    "koppen_df = countries_gdf.loc[:, [\"ISO3\", \"NAME\"]]\n",
    "koppen_df[\"koppen_stats\"] = stats\n",
    "koppen_df = pd.concat(\n",
    "    [koppen_df, koppen_df[\"koppen_stats\"].apply(pd.Series)], axis=1\n",
    ").fillna(0)\n",
    "pix_ct = pd.DataFrame(koppen_df.sum(axis=1))\n",
    "cat_pct = koppen_df.iloc[:, 3:].div(pix_ct[0], axis=0, fill_value=None)\n",
    "cat_pct = cat_pct.iloc[:, 1:]\n",
    "keep_cat = [int(x - 1) for x in cat_pct.columns]\n",
    "cat_pct.columns = list(koppen_codes[\"let\"].iloc[keep_cat])\n",
    "koppen_df = pd.concat([koppen_df.iloc[:, 0:2], cat_pct], axis=1)\n",
    "\n",
    "koppen_df = koppen_df.fillna(0)\n",
    "koppen_df.drop([\"NAME\"], axis=1, inplace=True)\n",
    "koppen_df = koppen_df.set_index(\"ISO3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koppen_df.to_csv(f\"{input_dir}/koppen_{mask}Mask{threshold_val}.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If loading from file:\n",
    "koppen_df = pd.read_csv(\n",
    "    f\"{input_dir}/koppen_{mask}Mask{threshold_val}.csv\", index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koppen_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "The step only needs to be run once. \n",
    "\n",
    "Using a binary host map, calculate the percent area in each country with\n",
    "probable presence of host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host file name\n",
    "host_file = \"toh.global_ensemble.tif\"\n",
    "\n",
    "# Read bindary host raster\n",
    "# Copy host raster in Data folder or update path as appropriate\n",
    "host_rast_path = f\"{data_dir}/{host_file}\"\n",
    "host_rast = rasterio.open(host_rast_path)\n",
    "\n",
    "# Check raster matches projection and resolution of other inputs\n",
    "if (host_rast.crs != \"EPSG:4326\") or (\n",
    "    host_rast.transform[0] != koppen_rast.transform[0]\n",
    "):\n",
    "    print(\"Resampling host raster to match climate raster...\")\n",
    "    prj_dst = f\"{os.path.splitext(host_rast_path)[0]}_prj.tif\"\n",
    "    dst_crs = koppen_rast.crs\n",
    "\n",
    "    with rasterio.open(host_rast_path) as src:\n",
    "        transform, width, height = rasterio.warp.calculate_default_transform(\n",
    "            src.crs,\n",
    "            dst_crs,\n",
    "            src.width,\n",
    "            src.height,\n",
    "            *src.bounds,\n",
    "            resolution=koppen_rast.res,\n",
    "        )\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update(\n",
    "            {\"crs\": dst_crs, \"transform\": transform, \"width\": width, \"height\": height}\n",
    "        )\n",
    "\n",
    "        with rasterio.open(prj_dst, \"w\", **kwargs) as dst:\n",
    "            rasterio.warp.reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=rasterio.band(dst, 1),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=dst_crs,\n",
    "                resampling=rasterio.warp.Resampling.nearest,\n",
    "            )\n",
    "        dst.close()\n",
    "    host_rast = rasterio.open(prj_dst)\n",
    "    host_arr = host_rast.read(1)\n",
    "    print(\"\\tdone\")\n",
    "else:\n",
    "    host_arr = host_rast.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set large negative values to 0\n",
    "host_arr[host_arr < 0.0001] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check values of original host raster\n",
    "host_arr_val_counts = np.unique(host_arr, return_counts=True)\n",
    "host_arr_val_counts[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If raster contains only two values, are they already 0 and 1?\n",
    "if len(host_arr_val_counts[0]) == 2:\n",
    "    if (host_arr_val_counts[0][0]) == 0 and (host_arr_val_counts[0][1] == 1):\n",
    "        print(\"Host raster is already in the required binary form\")\n",
    "    # If raster has only two values, but they are not 0 and 1, check output\n",
    "    else:\n",
    "        print(\"Host raster needs to be converted to a binary output\")\n",
    "        print(\"with 0 = no host present, 1 = host present\")\n",
    "        print(host_arr_val_counts[0])\n",
    "\n",
    "# If more than two values are present, convert based on specified threshold\n",
    "else:\n",
    "    print(\"Converting provided host raster to a binary output\")\n",
    "    threshold = 0.2  # e.g., above 20% if in percent, or >=1 if area harvested\n",
    "    host_arr = convert_to_binary(host_arr, threshold)\n",
    "\n",
    "# Confirm converted to 0 and 1\n",
    "print(np.unique(host_arr, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask == \"cl\":\n",
    "    host_arr = np.append(\n",
    "        host_arr,\n",
    "        np.zeros([cl_arr.shape[0] - host_arr.shape[0], host_arr.shape[1]]),\n",
    "        axis=0,\n",
    "    )\n",
    "    host = host_arr * cl_arr  # This might not work if they are very different shapes\n",
    "\n",
    "if mask == \"hii\":\n",
    "    host_arr = np.append(\n",
    "        host_arr,\n",
    "        np.zeros([hii_arr.shape[0] - host_arr.shape[0], host_arr.shape[1]]),\n",
    "        axis=0,\n",
    "    )\n",
    "    host = host_arr * hii_arr\n",
    "\n",
    "if mask == \"none\":\n",
    "    host = host_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate zonal statistics\n",
    "affine = host_rast.transform\n",
    "stats = zonal_stats(countries_gdf, host, categorical=True, affine=affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create host dataframe with country identifiers, results from zonal stats, and calculate host percent area\n",
    "host_df = countries_gdf.loc[:, [\"ISO3\", \"NAME\"]]\n",
    "host_df[\"host_stats\"] = stats\n",
    "host_df = pd.concat([host_df, host_df[\"host_stats\"].apply(pd.Series)], axis=1).fillna(0)\n",
    "host_df[\"Host Percent Area\"] = (host_df[1.0] / (host_df[0.0] + host_df[1.0])).fillna(0)\n",
    "host_df.iloc[136, 1] = \"Macao\"\n",
    "host_df.iloc[169, 1] = \"Réunion\"\n",
    "host_df.iloc[17, 1] = \"Myanmar\"\n",
    "host_df.iloc[245, 1] = \"Saint Barthelemy\"\n",
    "host_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df.to_csv(f\"{input_dir}/host_{mask}Mask{threshold_val}.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If loading from file:\n",
    "host_df = pd.read_csv(f\"{input_dir}/host_{mask}Mask{threshold_val}.csv\", sep=\",\")\n",
    "host_df.drop([\"Unnamed: 0\", \"host_stats\", \"0.0\", \"1.0\"], axis=1, inplace=True)\n",
    "host_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phytosanitary Capacity Data\n",
    "Incorporates an estimate of phytosanitary capacity for each country. Currently the model \n",
    "is using the proactive value from:\n",
    "\n",
    "Early, R., Bradley, B., Dukes, J. et al. Global threats from invasive alien species in the twenty-first century and national response capacities. Nat Commun 7, 12485 (2016). https://doi-org.prox.lib.ncsu.edu/10.1038/ncomms12485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyto_path = f\"{data_dir}/phytosanitary_capacity_iso3.csv\"  # Original file found in: Q:\\Shared drives\\APHIS  Projects\\Pandemic\\Data\\phytosanitary_capacity\n",
    "phyto_df = pd.read_csv(phyto_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyto_df = phyto_df[[\"proactive\", \"ISO3\", \"UN\"]]\n",
    "phyto_df = phyto_df.rename(columns={\"proactive\": \"Phytosanitary Capacity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyto_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final countries dataframe\n",
    "Merge country attributes with host percent area, climate classificaiton percent area, \n",
    "and phytosanitary capacity estimates. Filter and order dataframe to match countries \n",
    "with trade data available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge koppen, host, and phytosanitary attributes with countries geodataframe\n",
    "countries_gdf = countries_gdf.merge(koppen_df, on=\"ISO3\")\n",
    "countries_gdf = countries_gdf.merge(\n",
    "    host_df[[\"ISO3\", \"Host Percent Area\"]], how=\"left\", on=\"ISO3\"\n",
    ")\n",
    "countries_gdf = countries_gdf.merge(\n",
    "    phyto_df, how=\"left\", on=\"ISO3\", suffixes=[None, \"_y\"]\n",
    ")\n",
    "countries_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values of phytosanitary capacity for rescaling\n",
    "unique_keys = list(countries_gdf[\"Phytosanitary Capacity\"].unique())\n",
    "unique_keys.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale input phytosanitary capacity values using specified\n",
    "# minimum and maxmimum values\n",
    "\n",
    "# Minimum phytosanitary capacity value when rescaled\n",
    "scaled_min = 0.3\n",
    "\n",
    "# Maximum phytosanitary capacity value when rescaled\n",
    "scaled_max = 0.8\n",
    "\n",
    "phyto_dict = {}\n",
    "\n",
    "for i in unique_keys:\n",
    "    if np.isnan(i):\n",
    "        phyto_dict[i] = 0\n",
    "    else:\n",
    "        if np.isnan(unique_keys).any():\n",
    "            increments = len(unique_keys) - 1\n",
    "        else:\n",
    "            increments = len(unique_keys)\n",
    "\n",
    "        scale_diff = scaled_max - scaled_min\n",
    "        phyto_dict[i] = (\n",
    "            round((scale_diff / increments) * unique_keys.index(i), 2) + scaled_min\n",
    "        )\n",
    "\n",
    "phyto_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_gdf[\"Phytosanitary Capacity\"] = countries_gdf[\n",
    "    \"Phytosanitary Capacity\"\n",
    "].replace(phyto_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_gdf.set_index(\"ISO3\", inplace=True)\n",
    "countries_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read example trade matrix to identify which countries are in the geodataframe but not\n",
    "# in the trade data\n",
    "if temporal_res == \"M\":\n",
    "    temporal_folder = \"monthly\"\n",
    "    month_str = \"01\"\n",
    "if temporal_res == \"A\":\n",
    "    temporal_folder = \"annual\"\n",
    "    month_str = \"\"\n",
    "    \n",
    "example_trade = pd.read_csv(\n",
    "    f\"{input_dir}/comtrade/{temporal_folder}_{dir_suffix}/{code_str}/{code_str}_{start_year}{month_str}.csv\",\n",
    "    header=0,\n",
    "    index_col=0,\n",
    "    encoding=\"latin-1\",\n",
    ")\n",
    "country_set = set(countries_gdf.index.values)\n",
    "trade_set = set(example_trade.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# ISO3 codes in countries geopackage:\\t\", len(country_set))\n",
    "print(\"# ISO3 code matches:\\t\\t\\t\", len(trade_set.intersection(country_set)))\n",
    "\n",
    "print(\"Which countries are in the TRADE data but NOT the COUNTRIES geopackage\")\n",
    "miss_country = trade_set - country_set\n",
    "if miss_country:\n",
    "    print(\"\\n\", miss_country)\n",
    "else:\n",
    "    print(\"No missing countries.\")\n",
    "\n",
    "\n",
    "print(\"Which countries are in the COUNTRIES geopackage but NOT the TRADE data:\")\n",
    "miss_trade = country_set - trade_set\n",
    "if miss_trade:\n",
    "    print(\"\\n\", miss_trade)\n",
    "else:\n",
    "    print(\"No missing countries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMN: Isle of Man\n",
    "PRI: Puerto Rico\n",
    "ALA: Aland Islands\n",
    "LIE: Liechtenstein\n",
    "MTQ: Martinique\n",
    "GUF: French Guiana\n",
    "MAF: Saint-Martin (French part)\n",
    "TWN: Taiwan\n",
    "JEY: Jersey\n",
    "MCO: Monaco\n",
    "GGY: Guernsey\n",
    "GLP: Guadeloupe\n",
    "REU: Réunion\n",
    "VIR: US Virgin Islands\n",
    "BVT: Bouvet Island\n",
    "SJM: Svalbard and Jan Mayen Islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove countries from the geodataframe that do not have trade data\n",
    "countries_filtered = countries_gdf.drop(miss_trade, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the countries geodataframe rows to match the\n",
    "# trade index order\n",
    "index_list = list(example_trade.index.values)\n",
    "countries_filtered_reindex = countries_filtered.loc[index_list, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_filtered_reindex.reset_index(inplace=True)\n",
    "countries_filtered_reindex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Distance Matrix\n",
    "Calculate the distance between each origin-destination country pair. Save \n",
    "as matrix array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = distance_between(\n",
    "    example_trade, countries_filtered_reindex\n",
    ")  # great circle dist in km\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"{input_dir}/distance_matrix.npy\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Climate Simiarities Matrix\n",
    "Calculate the similarity matrix. Options for matching between each origin-destination country pair\n",
    "or matching to the origins at time step 1 only. Save as matrix array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_match_type = \"origins\"  # \"origins\" or \"pairs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_countries_list = [\n",
    "    \"China\",\n",
    "    \"Viet Nam\",\n",
    "]\n",
    "\n",
    "native_iso_list = []\n",
    "for country in native_countries_list:\n",
    "    native_iso_list.append(countries_gdf[countries_gdf[\"NAME\"] == country].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if climate_match_type == \"pairs\":\n",
    "    # Create an n x n array of climate similarity calculations\n",
    "    climate_similarities = create_climate_similarities_matrix(\n",
    "        array_template=example_trade, countries=countries_filtered_reindex\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if climate_match_type == \"origins\":\n",
    "    # If matching to origins at time step 1, set list of origins\n",
    "\n",
    "    # Specify if only areas with host in origins should be considered\n",
    "    host_only = False\n",
    "\n",
    "    if host_only:\n",
    "        # Mask to include only areas with host in calculations\n",
    "        koppen = koppen_arr * host_arr\n",
    "    else:\n",
    "        koppen = koppen_arr\n",
    "\n",
    "    # Calculate koppen categories for each country (must recompute here, without commodity destination mask)\n",
    "    stats = zonal_stats(countries_gdf, koppen, categorical=True, affine=affine)\n",
    "    # Add % of each climate classification to countries geodataframe\n",
    "    koppen_df = countries_gdf.loc[:, [\"NAME\"]]\n",
    "    koppen_df[\"koppen_stats\"] = stats\n",
    "    koppen_df = pd.concat(\n",
    "        [koppen_df, koppen_df[\"koppen_stats\"].apply(pd.Series)], axis=1\n",
    "    ).fillna(0)\n",
    "    pix_ct = pd.DataFrame(koppen_df.sum(axis=1))\n",
    "    cat_pct = koppen_df.iloc[:, 3:].div(pix_ct[0], axis=0, fill_value=None)\n",
    "    keep_cat = [int(x - 1) for x in cat_pct.columns]\n",
    "    cat_pct.columns = list(koppen_codes[\"let\"].iloc[keep_cat])\n",
    "    koppen_df = pd.concat([koppen_df.iloc[:, 0:2], cat_pct], axis=1)\n",
    "\n",
    "    koppen_df = koppen_df.fillna(0)\n",
    "    koppen_df.drop([\"NAME\", \"koppen_stats\"], axis=1, inplace=True)\n",
    "\n",
    "    native_iso_list = []\n",
    "    for country in native_countries_list:\n",
    "        native_iso_list.append(countries_gdf[countries_gdf[\"NAME\"] == country].index[0])\n",
    "\n",
    "    native_koppen = koppen_df.loc[native_iso_list]\n",
    "    origin_climates = native_koppen[native_koppen > 0].dropna(axis=1, thresh=1).columns\n",
    "\n",
    "    # Create an n x 1 array of climate similarity calculations\n",
    "    climate_similarities = create_climate_similarities_matrix_origins(\n",
    "        countries=countries_filtered_reindex, origins_climate_list=origin_climates\n",
    "    )\n",
    "\n",
    "    countries_filtered_reindex[\"Climate Similarity\"] = climate_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    f\"{input_dir}/climate_similarities_{mask}Mask{threshold_val}.npy\",\n",
    "    climate_similarities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered and reindexed countries data with climate, host %s, and phyto values\n",
    "countries_path = f\"{input_dir}/countries_{mask}Mask{threshold_val}.gpkg\"\n",
    "countries_filtered_reindex.to_file(countries_path, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full country data frame path to .env file\n",
    "print(dotenv.set_key(\".env\", \"COUNTRIES_PATH\", countries_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation and native countries dataframes\n",
    "\n",
    "Native countries: A list of the countries native range (provided above), determined from literature.\n",
    "\n",
    "Validation: Get country-year first observations from GBIF. This data can be supplemented with dates from literature and other sources. If no observations are available, you wil need to create the dataframe in the format provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create native countries list\n",
    "\n",
    "pd.Series(native_iso_list, name=\"ISO3\").to_csv(\n",
    "    input_dir + \"origin_locations.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation data\n",
    "\n",
    "species_sci_name = \"Lycorma delicatula\"\n",
    "method = \"GBIF\"  # \"GBIF\" or \"manual\"\n",
    "\n",
    "years = list(range(start_year, end_year + 1))\n",
    "\n",
    "# Or, set manual years:\n",
    "# years = list(range(1980,2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GBIF usageKey (unique identifier)\n",
    "if method == \"GBIF\":\n",
    "    gbif_key = get_GBIF_key(species_sci_name)\n",
    "\n",
    "# If the species is not found, you will need to create the observations dataframe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first records (year-country)\n",
    "\n",
    "if method == \"GBIF\":\n",
    "    first_records = get_GBIF_records(gbif_key, years)\n",
    "\n",
    "# If records are not found, you will need to create the observations dataframe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove native country records\n",
    "\n",
    "first_records = first_records.loc[\n",
    "    ~first_records[\"ISO3\"].isin(native_iso_list)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: manually create dataframe:\n",
    "if method == \"manual\":\n",
    "    countries_ISO3 = [\"KOR\", \"JPN\", \"USA\"]\n",
    "    first_intro_year = [2009, 2010, 2014]\n",
    "\n",
    "    first_records = pd.DataFrame(\n",
    "        {\"ISO3\": countries_ISO3, \"ObsFirstIntro\": first_intro_year}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it looks correct\n",
    "\n",
    "first_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to .csv\n",
    "\n",
    "first_records.to_csv(input_dir + \"first_records_validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save key data paremeters to the case study configuration file\n",
    "Further modified in 2. Create Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "# Trade data related\n",
    "\n",
    "args[\"timestep\"] = temporal_folder\n",
    "args[\"trade_type\"] = dir_suffix\n",
    "\n",
    "if agg_commodities == True:\n",
    "    args[\"commodity_list\"] = [code_str]\n",
    "else:\n",
    "    args[\"commodity_list\"] = hs_list\n",
    "\n",
    "# For accessing input file names\n",
    "\n",
    "args[\"climate_match\"] = climate_match_type\n",
    "args[\"climate_match_host_only\"] = host_only\n",
    "args[\"mask\"] = mask\n",
    "args[\"threshold_val\"] = threshold_val\n",
    "\n",
    "# Native countries\n",
    "\n",
    "args[\"native_countries_list\"] = native_iso_list\n",
    "\n",
    "# Phytosantitary capacity scales\n",
    "\n",
    "args[\"scaled_min\"] = scaled_min\n",
    "args[\"scaled_max\"] = scaled_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_json_path = f\"{out_dir}/config_{sim_name}.json\"\n",
    "\n",
    "if os.path.isfile(config_json_path):\n",
    "    with open(config_json_path) as file:\n",
    "        prev_config = json.load(file)\n",
    "\n",
    "    prev_config.update(args)\n",
    "\n",
    "    with open(config_json_path, mode=\"w\") as f:\n",
    "        f.write(json.dumps(prev_config, indent=4))\n",
    "\n",
    "else:\n",
    "    with open(config_json_path, \"w\") as file:\n",
    "        json.dump(args, file, indent=4)\n",
    "\n",
    "print(\"\\tSaved \", config_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Evaluate case study and data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "948a6e006881c847639198d4e28507cd0955feff6e008072919ba7456f12f8bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Pandemic': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
