{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoPS Global Model: Calibration \n",
    "Use this notebook to run and evaluate a parameter grid-search. This calibration process will use the parameter ranges defined in notebook 2 to generate a grid of parameter samples. The results of multiple stochastic model runs from each parameter set are then evaluated, and top performing sets are sampled in notebook 3c to generate a forecast. \n",
    "\n",
    "This notebook can be run after 0, 1, and 2. We recommend also running 3a first, to check for and troubleshoot issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace from env and configuration files \n",
    "\n",
    "Navigate to main repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate one level up to the main repository\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed PoPS Global functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandemic.multirun_helpers import write_commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in path variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read environmental variables\n",
    "env_file = os.path.join(\".env\")\n",
    "dotenv.load_dotenv(env_file)\n",
    "\n",
    "input_dir = os.getenv(\"INPUT_PATH\")\n",
    "out_dir = os.getenv(\"OUTPUT_PATH\")\n",
    "sim_name = os.getenv(\"SIM_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in parameter variables from config.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_json_path = f\"{out_dir}/config_{sim_name}.json\"\n",
    "\n",
    "with open(config_json_path) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# Read in parameters to calibrate\n",
    "\n",
    "alphas = config[\"alphas\"]\n",
    "betas = config[\"betas\"]\n",
    "lamdas = config[\"lamdas\"]\n",
    "start_years = config[\"start_years\"]\n",
    "\n",
    "start_run = config[\"start_run\"]\n",
    "end_run = config[\"end_run\"]\n",
    "\n",
    "validation_method = config[\"validation_method\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model in parallel\n",
    "\n",
    "Run the model over a range of parameter values to perform the calibration grid search. The model runs are \n",
    "sent using command line and are run in parallel. The results will print out after each command is run. \n",
    "\n",
    "The below commands create a list of all possible parameter combinations, and write out each as a command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all parameter combinations\n",
    "\n",
    "param_list = [alphas, betas, lamdas, start_years]\n",
    "param_sets = list(itertools.product(*param_list))\n",
    "\n",
    "# Write out commands\n",
    "\n",
    "commands_calibrate = \"\"\n",
    "\n",
    "for set in param_sets:\n",
    "    commands_calibrate += write_commands(\n",
    "        set, start_run=start_run, end_run=end_run, run_type=\"calibrate\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you will run on HPC or later, write these to file (un-comment the below code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = open(input_dir + \"/commands_calibrate.txt\", 'w')\n",
    "# f1.write(commands_calibrate)\n",
    "# f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to execute all model runs. These must complete before you can calculate the summary \n",
    "statistics. If you have set a wide range of parameter sets or have indicated many model runs, this may \n",
    "take some time (approximately 2 - 5 minutes per run per core, depending on your computer and number of \n",
    "time-steps in your simulation), so prepare accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model from script\n",
    "\n",
    "for command in commands_calibrate.split('\\n'):\n",
    "    print(f\"Running command: {command}\")\n",
    "    ! {command}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These runs will write out to \"outputs/{sim_name}_calibrate/\". \n",
    "\n",
    "Calculate summary statistics on completed runs. This is also run in parallel, so time will vary depending\n",
    "on how many cores you use. If you have many runs or many parameter samples, this may take some time as \n",
    "well (approximately 1 hour per 50,000 runs included, when run on 8 cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "# Note: The summary stats  may generate a \"warning\" from the pandas library. This should not cause any errors.\n",
    "\n",
    "! python pandemic/get_stats.py calibrate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate grid performance\n",
    "F-beta is the primary metric used to evaluate model run performance. The below visualizations help \n",
    "evaluate parameter sample convergence and performance according to this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dir = f\"{out_dir}/summary_stats/{sim_name}_calibrate\"\n",
    "\n",
    "validation_df = pd.read_csv(\n",
    "    input_dir + \"/first_records_validation.csv\", header=0, index_col=0,\n",
    ")\n",
    "\n",
    "col_dict = {\n",
    "    \"start_max\": \"start\",\n",
    "    \"alpha_max\": \"alpha\",\n",
    "    \"beta_max\": \"beta\",\n",
    "    \"lamda_max\": \"lamda\",\n",
    "}\n",
    "\n",
    "agg_df = pd.read_csv(f\"{stats_dir}/summary_stats_bySample.csv\").rename(columns=col_dict)\n",
    "stats = pd.read_csv(\n",
    "    f\"{stats_dir}/summary_stats_wPrecisionRecallF1FBetaAggProb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to save calibration figures\n",
    "\n",
    "fig_dir = f\"{stats_dir}/figs/calibration/\"\n",
    "\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization \n",
    "\n",
    "- Assess run convergence: Evaluate the top performing parameter samples to see if the F-beta score has \n",
    "converged. If it hasn't converged, you may need to conduct more runs. You can include additional runs \n",
    "by updating the start_run (to continue after previous end_run), end_run, and run_count. \n",
    "- Assess parameter set performance: Visualize F-beta across values for alpha, lamda, beta, and start \n",
    "year. Highest performing values should generally cluster towards a limited range for each parameter. \n",
    "If the highest performing values are at an extreme of a parameter range (e.g. at the highest end of \n",
    "lamda, at the earliest year), you may need to expand your grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess run convergence\n",
    "Do the individual lines (parameter samples) sufficiently converge with respect to F-beta over the \n",
    "number of runs conducted, or are more runs needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top N parameter samples\n",
    "\n",
    "n_samples = 5\n",
    "\n",
    "top_runs = agg_df.sort_values(\"fbeta_mean\", ascending=False).head(n_samples)\n",
    "fbeta_range = [min(top_runs.fbeta_mean) * 0.75, max(top_runs.fbeta_mean) * 1.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the performance of each sample with each additional run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(\n",
    "    agg_df.sort_values(\"fbeta_mean\", ascending=False).head(n_samples).reset_index()[\"sample\"]\n",
    ")\n",
    "runs = list(range(1, stats[\"run_num\"].max()+2))\n",
    "\n",
    "samples_df = pd.DataFrame({\"runs\": runs})\n",
    "i = 1\n",
    "for sample in samples:\n",
    "    sample_fbeta = []\n",
    "    stdev = []\n",
    "    sterr = []\n",
    "    for run in runs:\n",
    "        filtered_stats = stats.loc[\n",
    "            (stats[\"run_num\"] <= run - 1) & (stats[\"sample\"] == sample)\n",
    "        ]\n",
    "        value = filtered_stats[\"fbeta\"].mean()\n",
    "        sdev = filtered_stats[\"fbeta\"].std()\n",
    "        # this gives the standard deviation of the sample - mean\n",
    "        sample_fbeta.append(value)\n",
    "        stdev.append(sdev)\n",
    "        sterr.append(np.std(sample_fbeta))\n",
    "        # this gives the standard error of the mean\n",
    "    samples_df[f\"sample {i}\"] = sample_fbeta\n",
    "    samples_df[f\"stdev {i}\"] = stdev\n",
    "    samples_df[f\"sterr {i}\"] = sterr\n",
    "    i += 1\n",
    "\n",
    "samples_df.set_index(\"runs\", inplace=True)\n",
    "samples_df[\"all samples\"] = samples_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "ax = samples_df.loc[:, \"sample 1\":f\"sample {n_samples}\":3].plot(\n",
    "    ylim=fbeta_range,\n",
    "    cmap=\"mako\",\n",
    "    ylabel=\"fbeta\",\n",
    "    title=f\"Mean fbeta convergence \\n for the top {n_samples} parameter samples\",\n",
    "    legend=False,\n",
    ")\n",
    "for i in range(1, len(samples)):\n",
    "    ax.fill_between(\n",
    "        samples_df.index,\n",
    "        samples_df[f\"sample {i}\"] + samples_df[f\"sterr {i}\"],\n",
    "        samples_df[f\"sample {i}\"] - samples_df[f\"sterr {i}\"],\n",
    "        color=\"#366da0\",\n",
    "        alpha=0.15,\n",
    "    )\n",
    "ax.set_xlabel(\"# of Runs\", fontsize=16)\n",
    "ax.set_ylabel(\"Fbeta mean\", fontsize=16)\n",
    "ax.tick_params(labelsize=13)\n",
    "plt.savefig(f\"{fig_dir}/run_convergence.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing parameter set performance \n",
    "Once you have estalished that the sample F-beta has converged, you can assess the overall performance \n",
    "of different parameter sets to identify the highest performing sets. These visuals an also help you \n",
    "identify issues in your parameter sampling range. For instance, if your highest performing parameter \n",
    "samples all have the maximum lamda value in your sample range, you may need to increase the upper \n",
    "bounds of the values for that parameter.\n",
    "\n",
    "If leave-one-out cross validation is used, this will produce a visual per omitted validation location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(font_scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns with an Fbeta mean\n",
    "\n",
    "fbeta_cols = [col for col in agg_df.columns if \"fbeta\" in col and \"mean\" in col] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample performance (F-beta) by alpha value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fbeta_col in fbeta_cols:\n",
    "\n",
    "    ax = sns.stripplot(\n",
    "        x=\"alpha\",\n",
    "        y=fbeta_col,\n",
    "        hue=\"start\",\n",
    "        palette=\"mako\",\n",
    "        linewidth=0.2,\n",
    "        data=agg_df,\n",
    "        jitter=0.4,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles[::-1],\n",
    "        labels[::-1],\n",
    "        bbox_to_anchor=(1.25, 1),\n",
    "        loc=\"upper right\",\n",
    "        borderaxespad=0,\n",
    "        title=\"start year\",\n",
    "    )\n",
    "    ax.set(ylim=(0, 1))\n",
    "    ax.axes.set_title(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}, by Alpha Value\\n (Color = Year)\", fontsize=16)\n",
    "    ax.set_xlabel(\"Alpha\", fontsize=16)\n",
    "    ax.set_ylabel(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}\", fontsize=16)\n",
    "    ax.tick_params(labelsize=13)\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize=\"15\")  # for legend text\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize=\"15\")  # for legend title\n",
    "    plt.savefig(f\"{fig_dir}/{fbeta_col}_alpha.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample performance by lamda value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fbeta_col in fbeta_cols:\n",
    "\n",
    "    ax = sns.scatterplot(\n",
    "        x=\"lamda\",\n",
    "        y=fbeta_col,\n",
    "        hue=\"start\",\n",
    "        data=agg_df,\n",
    "        palette=\"mako\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.2,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "    ax.set(ylim=(0, 1))\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles[::-1],\n",
    "        labels[::-1],\n",
    "        bbox_to_anchor=(1.25, 1),\n",
    "        loc=\"upper right\",\n",
    "        borderaxespad=0,\n",
    "        title=\"start year\",\n",
    "    )\n",
    "    ax.axes.set_title(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}, by Lambda Value\\n (Color = Year)\", fontsize=16)\n",
    "    ax.set_xlabel(\"Lambda\", fontsize=16)\n",
    "    ax.set_ylabel(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}\", fontsize=16)\n",
    "    ax.tick_params(labelsize=13)\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize=\"15\")  # for legend text\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize=\"15\")  # for legend title\n",
    "    plt.savefig(f\"{fig_dir}/{fbeta_col}_lambda.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample performance by beta value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fbeta_col in fbeta_cols:\n",
    "\n",
    "    ax = sns.scatterplot(\n",
    "        x=\"beta\",\n",
    "        y=fbeta_col,\n",
    "        hue=\"start\",\n",
    "        data=agg_df,\n",
    "        palette=\"mako\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.2,\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "    ax.set(ylim=(0, 1))\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles[::-1],\n",
    "        labels[::-1],\n",
    "        bbox_to_anchor=(1.25, 1),\n",
    "        loc=\"upper right\",\n",
    "        borderaxespad=0,\n",
    "        title=\"start year\",\n",
    "    )\n",
    "    ax.axes.set_title(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}, by Beta Value\\n (Color = Year)\", fontsize=16)\n",
    "    ax.set_xlabel(\"Lambda\", fontsize=16)\n",
    "    ax.set_ylabel(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}\", fontsize=16)\n",
    "    ax.tick_params(labelsize=13)\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize=\"15\")  # for legend text\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize=\"15\")  # for legend title\n",
    "    plt.savefig(f\"{fig_dir}/{fbeta_col}_beta.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample performance by start year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fbeta_col in fbeta_cols:\n",
    "\n",
    "    ax = sns.stripplot(\n",
    "        x=\"start\",\n",
    "        y=fbeta_col,\n",
    "        hue=\"alpha\",\n",
    "        palette=\"mako\",\n",
    "        linewidth=0.2,\n",
    "        data=agg_df,\n",
    "        jitter=0.3,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles[::-1],\n",
    "        labels[::-1],\n",
    "        bbox_to_anchor=(1.25, 1),\n",
    "        loc=\"upper right\",\n",
    "        borderaxespad=0,\n",
    "        title=\"alpha\",\n",
    "    )\n",
    "    ax.set(ylim=(0, 1))\n",
    "    ax.axes.set_title(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}, by Start Year\\n (Color = Alpha)\", fontsize=16)\n",
    "    ax.set_xlabel(\"Start year\", fontsize=16)\n",
    "    ax.set_ylabel(f\"Fbeta {' '.join(fbeta_col.split('_')[1:])}\", fontsize=16)\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize=\"15\")  # for legend text\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize=\"15\")  # for legend title\n",
    "    plt.savefig(f\"{fig_dir}/{fbeta_col}_start.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Model run - Forecast\n",
    "\n",
    "If you are satisfied with the convergence and the parameter performance in the range of your grid, \n",
    "you can use these values to fit a parameter distribution that will be sampled from to conduct \n",
    "the forecast. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5263cf785d9856e1c25f07618a8816d1a19b937a7dc164fd109bb133e6c86867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
