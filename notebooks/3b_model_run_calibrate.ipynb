{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model run: Calibration \n",
    "Use this notebook to run and evaluate a parameter grid-search. \n",
    "\n",
    "This notebook can be run after 0, 1, and 2. We recommend also running 3a first, to check for and troubleshoot issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace from env and configuration files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate one level up to the main repository\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandemic.multirun_helpers import write_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read environmental variables\n",
    "env_file = os.path.join('.env') \n",
    "dotenv.load_dotenv(env_file)\n",
    "\n",
    "input_dir = os.getenv('INPUT_PATH')\n",
    "out_dir = os.getenv('OUTPUT_PATH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "run_name = config['sim_name']\n",
    "total_runs = config[\"run_count\"] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model in parallel\n",
    "\n",
    "Run the model over a range of parameter values to perform the calibration grid search. The model runs are sent using command line and are run in parallel. The results will print out after each command is run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in parameters to calibrate\n",
    "\n",
    "alphas = config[\"alphas\"]\n",
    "betas = config[\"betas\"]\n",
    "lamdas = config[\"lamdas\"]\n",
    "start_years = config[\"start_years\"]\n",
    "\n",
    "start_run = config[\"start_run\"]\n",
    "end_run = config[\"end_run\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all parameter combinations \n",
    "\n",
    "param_list = [alphas, betas, lamdas, start_years]\n",
    "param_sets = list(itertools.product(*param_list))\n",
    "\n",
    "# Write out commands\n",
    "\n",
    "commands_calibrate = \"\"\n",
    "\n",
    "for set in param_sets:\n",
    "    commands_calibrate += write_commands(set, start_run = start_run, end_run = end_run, run_type = \"calibrate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you will run on HPC or later, write these to file \n",
    "\n",
    "# f1 = open(stats_dir + \"/commands.txt\", 'w')\n",
    "# f1.write(commands_forecast)\n",
    "# f1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model from script\n",
    "\n",
    "for command in commands_calibrate.split('\\n'):\n",
    "    print(f\"Running command: {command}\")\n",
    "    ! {command}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "\n",
    "! python pandemic/get_stats.py calibrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate grid performance\n",
    "F-beta is the primary metric used to evaluate model run performance.\n",
    "\n",
    "Use the below to visualize and assess the...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dir = f\"{out_dir}/summary_stats/{run_name}_calibrate\"\n",
    "\n",
    "validation_df = pd.read_csv(\n",
    "        input_dir + \"/first_records_validation.csv\",\n",
    "        header=0,\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "col_dict = {\"start_max\":\"start\",\"alpha_max\":\"alpha\",\"beta_max\":\"beta\",\n",
    "    \"lamda_max\":\"lamda\",\"count_known_countries_time_window_fbeta_mean\":\"fbeta\"}\n",
    "\n",
    "agg_df = (\n",
    "    pd.read_csv(f\"{stats_dir}/summary_stats_bySample.csv\")\n",
    "    .rename(columns=col_dict)\n",
    "    )\n",
    "stats = (pd.read_csv(f\"{stats_dir}/summary_stats_wPrecisionRecallF1FBetaAggProb.csv\")\n",
    ".rename(columns=col_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization \n",
    "\n",
    "- Assess run convergence: Evaluate the top performing parameter samples to see if the F-beta score has converged. If it hasn't converged, you need to conduct more runs. You can include additional runs by updating the start_run (to continue after previous end_run), end_run, and run_count. \n",
    "- Assess parameter set performance: Visualize F-beta across values for alpha, lamda, beta, and start year. Highest performing values should generally cluster towards a limited range for each parameter. If the highest performing values are at an extreme of a parameter range (e.g. at the highest end of lamda, at the earliest year), you may need to expand your grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess run convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top N runs\n",
    "\n",
    "n_runs = 20\n",
    "\n",
    "top_runs = agg_df.sort_values('fbeta',ascending=False).head(n_runs)\n",
    "fbeta_range = [min(top_runs.fbeta)*0.75, max(top_runs.fbeta)*1.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(agg_df.sort_values('fbeta',ascending=False).head(n_runs).reset_index()['sample'])\n",
    "runs = list(range(0,stats['run_num'].max()))\n",
    "\n",
    "samples_df = pd.DataFrame({'runs':runs})\n",
    "i=1\n",
    "for sample in samples:\n",
    "    sample_fbeta = []\n",
    "    stdev = []\n",
    "    sterr = []\n",
    "    for run in runs:\n",
    "        filtered_stats = stats.loc[(stats['run_num']<=run) & (stats['sample']==sample)]\n",
    "        value = filtered_stats[\"count_known_countries_time_window_fbeta\"].mean()\n",
    "        sdev = filtered_stats[\"count_known_countries_time_window_fbeta\"].std() \n",
    "        # this gives the standard deviation of the sample - mean\n",
    "        sample_fbeta.append(value)\n",
    "        stdev.append(sdev)\n",
    "        sterr.append(np.std(sample_fbeta)) \n",
    "        # this gives the standard error of the mean\n",
    "    samples_df[f\"sample {i}\"]=sample_fbeta\n",
    "    samples_df[f\"stdev {i}\"]=stdev\n",
    "    samples_df[f\"sterr {i}\"]=sterr\n",
    "    i += 1\n",
    "\n",
    "samples_df.set_index(\"runs\",inplace=True)    \n",
    "samples_df[\"all samples\"]=samples_df.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "ax = (samples_df.loc[:,'sample 1':f'sample {n_runs}':3]\n",
    "    .plot(ylim=fbeta_range,cmap=\"mako\",ylabel=\"fbeta\",\n",
    "    title=\"Mean fbeta convergence \\n for the top 20 parameter samples\",\n",
    "    legend=False))\n",
    "for i in range(1, len(samples)):\n",
    "    ax.fill_between(samples_df.index, samples_df[f\"sample {i+1}\"]+samples_df[f\"sterr {i+1}\"], samples_df[f\"sample {i+1}\"]-samples_df[f\"sterr {i+1}\"],color='#366da0',alpha=0.15)\n",
    "ax.set_xlabel(\"# of Runs\",fontsize=16)\n",
    "ax.set_ylabel(\"Fbeta mean\",fontsize=16)\n",
    "ax.tick_params(labelsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing parameter set performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(font_scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.stripplot(x='alpha', y='fbeta', hue='start', palette='mako',linewidth=0.2, data=agg_df, jitter=0.4)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1],bbox_to_anchor=(1.25,1), loc='upper right', borderaxespad=0,title=\"start year\")\n",
    "ax.set(ylim=(0,1))\n",
    "ax.axes.set_title(\"Mean Sample Fbeta, by Alpha Value\\n (Color = Year)\",fontsize=16)\n",
    "ax.set_xlabel(\"Alpha\",fontsize=16)\n",
    "ax.set_ylabel(\"Fbeta mean\",fontsize=16)\n",
    "ax.tick_params(labelsize=13)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='15') # for legend title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='lamda',y='fbeta',hue='start',data=agg_df,palette='mako',edgecolor=\"black\",linewidth=0.2,legend='full') \n",
    "ax.set(ylim=(0, 1))\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1],bbox_to_anchor=(1.25,1), loc='upper right', borderaxespad=0,title=\"start year\")\n",
    "ax.axes.set_title(\"Mean Sample Fbeta, by Lambda Value\\n (Color = Year)\",fontsize=16)\n",
    "ax.set_xlabel(\"Lambda\",fontsize=16)\n",
    "ax.set_ylabel(\"Fbeta mean\",fontsize=16)\n",
    "ax.tick_params(labelsize=13)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='15') # for legend title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.stripplot(x='start', y='fbeta', hue='alpha', palette='mako',linewidth=0.2, data=agg_df, jitter=0.3)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1],bbox_to_anchor=(1.25,1), loc='upper right', borderaxespad=0,title=\"alpha\")\n",
    "ax.set(ylim=(0,1))\n",
    "ax.axes.set_title(\"Mean Sample Fbeta, by Start Year\\n (Color = Alpha)\",fontsize=16)\n",
    "ax.set_xlabel(\"Start year\",fontsize=16)\n",
    "ax.set_ylabel(\"Fbeta mean\",fontsize=16)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='15') # for legend title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Model run - Forecast\n",
    "\n",
    "If you are satisfied with the convergence and the parameter performance in the range of your grid, you can use these values to fit a parameter distribution that will be sampled from to conduct the forecast. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "948a6e006881c847639198d4e28507cd0955feff6e008072919ba7456f12f8bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Pandemic': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
