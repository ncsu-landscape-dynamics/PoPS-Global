{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study Evaluation \n",
    "\n",
    "This notebook uses exploratory data visualization and statistical models to evaluate the viability of the selected case study and the driver data provided. This notebook will use the data inputs and directories generated by the 1_data_acquisition_format notebook. \n",
    "\n",
    "You can use the visualizations and model results to assess:\n",
    "\n",
    "* Whether the commodity pathways explain the observed introductions\n",
    "* How prominent to expect bridgehead introductions to be\n",
    "* How to expect to weight establishment vs. entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace \n",
    "import os\n",
    "import glob\n",
    "import dotenv\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from functools import reduce\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GLM \n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environmental Variables and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook was launched from notebook folder of the clone GitHub\n",
    "# repository, then set working directory to level above\n",
    "# (e.g., '..' to navigate to /PoPS-Global)\n",
    "\n",
    "# This should be the path where the .env file is saved\n",
    "repo_path = str(input())\n",
    "os.chdir(repo_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables and paths from .env\n",
    "dotenv.load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to case-study specific raw data inputs (host map, phytosanitary capacity dataset)\n",
    "data_dir = os.getenv(\"DATA_PATH\")\n",
    "\n",
    "# Path to formatted model inputs\n",
    "input_dir = os.getenv(\"INPUT_PATH\")\n",
    "\n",
    "# Path to save outputs\n",
    "out_dir = os.getenv(\"OUTPUT_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the all driver data and create a static tabular format (row = country, column = feature)\n",
    "# Country dataframe, host area, phytosanitary capacity\n",
    "countries_gdf = gpd.read_file(glob.glob(input_dir + \"countries*.gpkg\")[0])\n",
    "\n",
    "# Validation data\n",
    "validation = pd.read_csv(glob.glob(input_dir +'*first_records_validation.csv')[0])\n",
    "validation = validation.loc[validation['ISO3'].isin(countries_gdf['ISO3'])]\n",
    "\n",
    "# Origin data\n",
    "origins = pd.read_csv(input_dir + 'origin_locations.csv')\n",
    "origins = origins.loc[origins['ISO3'].isin(countries_gdf['ISO3'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary of origins\n",
    "countries_gdf.loc[countries_gdf['ISO3'].isin(origins.ISO3),'Origin'] = 1\n",
    "countries_gdf.loc[~countries_gdf['ISO3'].isin(origins.ISO3),'Origin'] = 0\n",
    "\n",
    "# Binary of destinations\n",
    "countries_gdf.loc[countries_gdf['ISO3'].isin(validation.ISO3),'Destination'] = 1\n",
    "countries_gdf.loc[~countries_gdf['ISO3'].isin(validation.ISO3),'Destination'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate similarities matrix\n",
    "climate_similarities = np.load(glob.glob(input_dir + \"climate_similarities*.npy\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each country, what is the max similarity to all pest origin locations?\n",
    "countries_gdf['Climate_Max'] = np.max(climate_similarities[countries_gdf['Origin']==1,], axis=0)\n",
    "\n",
    "# Option to run mean similarity:\n",
    "countries_gdf['Climate_Mean'] = np.mean(climate_similarities[countries_gdf['Origin']==1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance matrix\n",
    "distance = np.load(glob.glob(input_dir + \"distance_matrix*.npy\")[0])\n",
    "\n",
    "# Set the diagonal (self-distances) to NaN\n",
    "np.fill_diagonal(distance, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each country, what is the minimum distance to a pest origin or bridgehead location?\n",
    "countries_gdf['Dist_Origin'] = np.nanmin(distance[countries_gdf['Origin']==1], axis=0)\n",
    "countries_gdf['Dist_Origin'] = MinMaxScaler().fit_transform(np.array(countries_gdf['Dist_Origin']).reshape(-1,1))\n",
    "\n",
    "countries_gdf['Dist_Bridge'] = np.nanmin(distance[countries_gdf['Destination']==1], axis=0)\n",
    "countries_gdf['Dist_Bridge'] = MinMaxScaler().fit_transform(np.array(countries_gdf['Dist_Bridge']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade - two columns per commodity\n",
    "\n",
    "# Total cumulative imports from (1) all source countries\n",
    "# Total cumulative imports from (2) all validation countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of time for validation data\n",
    "start_year = validation['ObsFirstIntro'].min() - 5\n",
    "end_year = validation['ObsFirstIntro'].max()\n",
    "\n",
    "# If these are beyond the bounds of your trade data, overwrite with static years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total trade data by commodity during the period of interest\n",
    "# From source countries\n",
    "# From validation countries (ie. potential bridgeheads)\n",
    "\n",
    "year_range = list(range(start_year, end_year + 1, 1))\n",
    "commodities = os.listdir(input_dir + 'comtrade/monthly_adjusted/')\n",
    "\n",
    "for commodity in commodities: \n",
    "    \n",
    "    try:\n",
    "        del trade_sum\n",
    "    except:\n",
    "        print(\"Initializing...\")\n",
    "    for d in year_range:\n",
    "        d_file_list = glob.glob(input_dir + f'/comtrade/monthly_adjusted/{commodity}/*_{d}*.csv')\n",
    "        print(f'{commodity}, {d}: {len(d_file_list)}')\n",
    "        dfs = [pd.read_csv(f, sep = \",\", header= 0, index_col=0, encoding='latin1') for f in d_file_list]\n",
    "        all_com = reduce(pd.DataFrame.add, dfs)\n",
    "        try:\n",
    "            trade_sum += all_com\n",
    "            print('Added to trade_sum')\n",
    "        except:\n",
    "            trade_sum = all_com\n",
    "            print('Created trade_sum')\n",
    "    \n",
    "    # Keep only origin exporters and other validation countries (bridgeheads)\n",
    "    countries_gdf[f'Origin_{commodity}'] = trade_sum[origins.ISO3].sum(axis=1).reset_index(drop=True)\n",
    "    countries_gdf[f'Origin_{commodity}'] = MinMaxScaler().fit_transform(np.array(countries_gdf[f'Origin_{commodity}']).reshape(-1,1))\n",
    "    countries_gdf[f'Bridge_{commodity}'] = trade_sum[validation.ISO3].sum(axis=1).reset_index(drop=True)\n",
    "    countries_gdf[f'Bridge_{commodity}'] = MinMaxScaler().fit_transform(np.array(countries_gdf[f'Bridge_{commodity}']).reshape(-1,1))\n",
    "    print(f'{commodity} summed.\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep host area and phytosanitary capacity as is\n",
    "countries_gdf['Host_Area'] = countries_gdf['Host Percent Area']\n",
    "countries_gdf['P_Cap'] = countries_gdf['Phytosanitary Capacity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simplified dataframe\n",
    "regression_df = countries_gdf.set_index('ISO3')\n",
    "regression_df = regression_df.loc[:,'Origin':].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out to .csv for future use\n",
    "regression_df.to_csv(input_dir + \"regression_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've already done the setup, read back in the regression data \n",
    "regression_df = pd.read_csv(input_dir + \"noTWN/regression_data.csv\")\n",
    "\n",
    "# Origin countries are excluded from this analysis\n",
    "regression_data = regression_df.loc[regression_df['Origin']==0].reset_index(drop=True).drop(columns=\"Origin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "sns.pairplot(regression_data, hue = \"Destination\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points for interpretation: \n",
    "\n",
    "- Do the introduced locations separate clearly from the rest of the data on any of the pathway variables (ie. traded commodities)? (esp. high values)\n",
    "- Does the relationship appear different with trade from origin vs. bridgeheads?\n",
    "- Is there a clear relationship with distance, indicating a role of cross-border spread or natural dispersal? (esp. low values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis: Binomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regression formula\n",
    "predictors = regression_data.loc[:,'Climate_Max':'P_Cap'].columns\n",
    "predictor_string = ' + '.join(predictors)\n",
    "\n",
    "print(f'Regression formula: Destination ~ {predictor_string}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data matrices\n",
    "y, X = dmatrices(f'Destination ~ {predictor_string}', data=regression_data, return_type='dataframe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run binomial regression model for binary presence outcome\n",
    "binomial_model = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "binomial_results = binomial_model.fit()\n",
    "print(binomial_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points for interpretation:\n",
    "\n",
    "- Because the PoPS Global simulation model treats drivers mechanistically, predictors should *positively* correlate with pest introduction, EXCEPT for distance \n",
    "- Consider removing commodity pathways with negative values\n",
    "- Strong negative correlation with distance may suggest a role for natural dispersal or cross border transportation beyond what is captured through trade\n",
    "- Highly predictive variables may cause \"Perfect separation\" error - if this is the case, you can use the [RJAGS version of the model](https://github.com/arielsaffer/pandemic-statespace)\n",
    "- Be cautious on interpretation due to the presence of confounding variables. Note that as this is a static treatment of trade volume (cumulative), the presence of high volume events or changes to volume over time will be missed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis: Random Forest (all predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.Destination.values, test_size=0.3) \n",
    "\n",
    "# Fit the model\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict to assess accuracy\n",
    "y_pred=clf.predict(X_test)\n",
    "print(f\"Accuracy score: {metrics.accuracy_score(y_test, y_pred)}; \\nReal positives: {y_test.sum()} (Predicted as: {y_pred[y_test==1]}) \\nPredicted positives: {y_pred.sum()} (Real values: {y_test[y_pred==1]})\")\n",
    "\n",
    "# Be wary that for case studies with few known introductions, this model may be \"accurate\" by producing all 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess feature importance\n",
    "feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance - within the original model \n",
    "sns.barplot(x=feature_imp, y=feature_imp.index, palette=\"mako\")\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"All Important Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importance within the test data\n",
    "result = permutation_importance(\n",
    "    clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(\n",
    "    result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx]\n",
    ")\n",
    "ax.set_title(\"Permutation Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points for interpretation:\n",
    "\n",
    "- Importance is relevant, BUT, we don't see here if the relationship is positive or negative\n",
    "- - E.g. countries importing tomatoes may be LESS likely to have the virus, because they produce fewer tomatoes\n",
    "- To help with this, try the RF with preselected predictors below, informed by the binomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis: Random Forest (pre-selected predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only positive pathways/negative distance predictors\n",
    "\n",
    "# Positive predictors - host, area,\n",
    "positive_predictors = list(binomial_results.params.index[binomial_results.params>0])\n",
    "\n",
    "# Except for distance which is relevant if negative\n",
    "for col in ['Dist_Origin','Dist_Bridge']:\n",
    "    if binomial_results.params[col] < 0:\n",
    "        positive_predictors.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[positive_predictors], y.Destination.values, test_size=0.3) \n",
    "\n",
    "# Fit the model\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict to assess accuracy\n",
    "y_pred=clf.predict(X_test)\n",
    "print(f\"Accuracy score: {metrics.accuracy_score(y_test, y_pred)}; \\nReal positives: {y_test.sum()} (Predicted as: {y_pred[y_test==1]}) \\nPredicted positives: {y_pred.sum()} (Real values: {y_test[y_pred==1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess feature importance\n",
    "feature_imp = pd.Series(clf.feature_importances_,index=X[positive_predictors].columns).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index, palette=\"mako\")\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Important Features \\nCommodities and Host/Climate +; Distance - \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(\n",
    "    result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx]\n",
    ")\n",
    "ax.set_title(\"Permutation Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data visualization and outcomes of the models, determine if the pathway drivers (traded commodities) are appropriate predictors of pest transport. If additional sources are needed, acquire with notebook 1 and re-run this analysis. \n",
    "\n",
    "If commodities do not appear to positively drive pest presence, consider removing them, especially if you will be aggregating multiple commodities.\n",
    "\n",
    "Once you have selected reasonable drivers, proceed to model calibration."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "948a6e006881c847639198d4e28507cd0955feff6e008072919ba7456f12f8bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Pandemic': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
